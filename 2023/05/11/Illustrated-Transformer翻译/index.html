<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Illustrated Transformer翻译 | Skylark Space</title><meta name="author" content="Ryan Wong"><meta name="copyright" content="Ryan Wong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="The Illustrated Transformer翻译  前言：Transformer的论文写的非常简洁，以至于有很多东西并不是很清楚，包括整个Transformer是怎么训练怎么运作的。这里借着对本文的翻译，加深理解。 原文：The Illustrated Transformer 参考翻译：图解transformer | The Illustrated Transformer  A High">
<meta property="og:type" content="article">
<meta property="og:title" content="Illustrated Transformer翻译">
<meta property="og:url" content="http://ryan-nightwish.github.io/2023/05/11/Illustrated-Transformer%E7%BF%BB%E8%AF%91/index.html">
<meta property="og:site_name" content="Skylark Space">
<meta property="og:description" content="The Illustrated Transformer翻译  前言：Transformer的论文写的非常简洁，以至于有很多东西并不是很清楚，包括整个Transformer是怎么训练怎么运作的。这里借着对本文的翻译，加深理解。 原文：The Illustrated Transformer 参考翻译：图解transformer | The Illustrated Transformer  A High">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://ryan-nightwish.github.io/img/default_cover.png">
<meta property="article:published_time" content="2023-05-11T11:31:59.000Z">
<meta property="article:modified_time" content="2023-05-14T12:04:01.491Z">
<meta property="article:author" content="Ryan Wong">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="论文">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://ryan-nightwish.github.io/img/default_cover.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://ryan-nightwish.github.io/2023/05/11/Illustrated-Transformer%E7%BF%BB%E8%AF%91/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Illustrated Transformer翻译',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-05-14 20:04:01'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/styles.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/static-butterfly/dist/css/index.min.css"><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/default_cover.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Skylark Space"><span class="site-name">Skylark Space</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Illustrated Transformer翻译</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-11T11:31:59.000Z" title="发表于 2023-05-11 19:31:59">2023-05-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-05-14T12:04:01.491Z" title="更新于 2023-05-14 20:04:01">2023-05-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Illustrated Transformer翻译"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="the-illustrated-transformer翻译">The Illustrated
Transformer翻译</h1>
<blockquote>
<p>前言：Transformer的论文写的非常简洁，以至于有很多东西并不是很清楚，包括整个Transformer是怎么训练怎么运作的。这里借着对本文的翻译，加深理解。</p>
<p>原文：<a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">The
Illustrated Transformer</a></p>
<p>参考翻译：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36667170/article/details/124359818">图解transformer
| The Illustrated Transformer</a></p>
</blockquote>
<h2 id="a-high-level-look">A High-Level Look</h2>
<p>我们可以把模型看作是一个黑盒。在机器翻译的应用中，它会把一个句子从一个语言翻译到另一个语言。</p>
<figure>
<img src="the_transformer_3-20230511194041318.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>打开这个黑盒，我们可以看到一个编码器和一个解码器，二者之间存在联系。</p>
<figure>
<img src="The_transformer_encoders_decoders.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>编码器部分是六个编码器堆叠在一起（论文里是六个，可以选择其他数量）。解码器也是一样，是六个解码器的堆叠。</p>
<p><img src="The_transformer_encoder_decoder_stack.png" alt="img" style="zoom:67%;"></p>
<blockquote>
<p>注意这里的结构图，前五个encoder的输出都只是传递给下一个encoder，最后一个encoder的输出会传给decoder。</p>
</blockquote>
<p>每一个编码器都是相同的结构（但是它们的参数并不共享）。每个编码器可以拆分为两个子层：</p>
<figure>
<img src="Transformer_encoder.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>编码器的输出会首先进入一个自注意力层，在编码某个单词时，自注意力层可以同时看到输入句子中的其他单词。这一点后面会详细说。</p>
<p>自注意力层的输出会进入一个前馈神经网络，在每个自注意力层的输出部分都会有这样结构相同的的前馈神经网络。</p>
<p>解码器有编码器的层次结构，但在它们之间还有一个注意力层，这个注意力层帮助解码器注意到输入句子的相关部分（和seq2seq模型中的注意力相似）。</p>
<figure>
<img src="Transformer_decoder.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="bringing-the-tensors-into-the-picture">Bringing The Tensors Into
The Picture</h2>
<p>现在我们进入这个模型的主要部分，我们将以张量或向量为对象，看看它在模型中是怎么变化得，以及输入是如何变成输出的。</p>
<p>首先，我们使用嵌入算法来将输入的单词转化为向量：</p>
<figure>
<img src="embeddings.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<blockquote>
<p>假设每个词嵌入的维度是512维，这里仅用四个方格来表示512维</p>
</blockquote>
<p>嵌入只发生在编码器的最底层，也就是输入编码器的时候。每一个编码器会接收一个由512维向量组成的列表，对于最底层的编码器来说，它接收的是生成的词向量，但是对于其他上层的编码器，它们接收的是下一层编码器的输出。输入嵌入的大小是可以设置的超参数，一般而言会设置成训练集中最长句子的长度。</p>
<p>当输入的句子进行嵌入后，它们会依次进入编码器的两个子层：</p>
<p><img src="encoder_with_tensors.png" alt="img" style="zoom:67%;"></p>
<p>这里我们可以看到Transformer的一个关键属性，即每个位置上的单词在编码器中都有自己的运行路线。在自注意力层中，这些路线之间会存在依赖关系，但在前馈神经网络中却没有依赖关系，因此所有路线在通过前馈神经网络时是可以并行计算的。</p>
<p>下面我们用一个短句举例，看看在编码器的每个词层中都发生了什么。</p>
<h2 id="now-were-encoding">Now We're Encoding！</h2>
<p>正如我们所说的，一个编码器接受一个向量列表作为输入。它将这个列表中的向量传递到自注意力层后进入前馈神经网络，然后将输出传递给下一个编码器：</p>
<p><img src="encoder_with_tensors_2.png" alt="img" style="zoom:67%;"></p>
<h2 id="self-attention-at-a-high-level">Self-Attention at a High
Level</h2>
<p>不要一看“self-attention”就觉得这是个每个人都很很熟悉的词，其实我个人感觉，在看《Attention
is all you
need》之前我都没有真正理解自注意力机制。现在让我们看一下自注意力机制。</p>
<p>假设我们想要翻译下面这句话：</p>
<p><code>The animal didn't cross the street because it was too tired</code></p>
<p>这个句子里的<code>it</code>是指什么？它是指<code>street</code>还是<code>animal</code>？这对人来说不是问题，但对机器来说并不容易。</p>
<p>当模型处理到<code>it</code>的时候，自注意力可以把<code>it</code>和<code>animal</code>关联起来。</p>
<p>当模型处理句子中的每个单词时，自注意力都会在这个句子的其他位置寻找线索，从而可以让这个单词被编码得更好。</p>
<p>如果你对RNN比较熟悉的话，可以回想一下RNN中保持隐藏状态是如何将之前处理过的向量和现在正在处理的向量建立联系的。而在Transformer中，自注意力也是这样，可以将其他相关单词的“理解”融入到当前单词的处理中。</p>
<figure>
<img src="transformer_self-attention_visualization.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<blockquote>
<p>当我们在第五个（最后一个）编码器中处理<code>it</code>的时候，会发现注意力机制更关注“The
animal”，并会把对其的理解融入到对<code>it</code>的编码中。</p>
</blockquote>
<p>上面的可视化图是官方的<a target="_blank" rel="noopener" href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb">Tensor2Tensor
notebook</a>。</p>
<h2 id="self-attention-in-detail">Self-Attention in Detail</h2>
<p>下面看看怎样使用向量计算自注意力，再看一下实际中是怎么使用矩阵来实现的。</p>
<p>自注意力计算中的第一步是对每个编码器的输入向量计算出三个向量，或者说，我们对每个输入向量都创建一个Query向量、一个Key向量和一个Value向量。三个向量是通过将输入向量分别乘上三个权重矩阵（<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="13.981ex" height="2.386ex" role="img" focusable="false" viewBox="0 -860.8 6179.6 1054.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1136.2,363) scale(0.707)"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g></g><g data-mml-node="mo" transform="translate(1745.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msup" transform="translate(2190.2,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1136.2,363) scale(0.707)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><g data-mml-node="mo" transform="translate(4005,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msup" transform="translate(4449.7,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1136.2,363) scale(0.707)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></g></svg></mjx-container></span>）得到的，三个权重矩阵会在训练过程中训练出来。</p>
<p>注意在这里，新的向量会比嵌入向量小一些，它们都是64维，而输入向量和编码器的输入输出向量都是512维。这三个向量并不是必须比编码器输入输出的维度小，这样做是为了让多头注意力的计算更稳定。</p>
<p><img src="transformer_self_attention_vectors.png" alt="img" style="zoom:67%;"></p>
<p>那么query、key和value向量是什么呢？</p>
<p>它们是计算注意力时的抽象概念，往下看，看完就懂了。</p>
<p>计算自注意力的第二步是计算注意力得分。假设现在我们要计算输入的第一个单词“Thinking”的自注意力，我们需要根据当前单词给输入序列的每个单词打分，这个分数决定了当我们处理到当前单词时，应该给其他位置上的单词付出多少关注。</p>
<p>这个得分是通过query向量和key向量的点积求出来的。所以当我们处理到第一个位置的单词的自注意力时，第一个得分就是<code>q1</code>和<code>k1</code>的点积，第二个得分就是<code>q1</code>和<code>k2</code>的点积：</p>
<figure>
<img src="transformer_self_attention_score.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>第三、四步是将分数结果除以8（论文中key向量的维度是64，而8是64的平方根。这样可以让梯度计算的时候更稳定。可以取其他值，这是默认设定），然后将结果进行softmax计算。softmax可以将注意力分数归一化，使得分数均为正数且和为一：</p>
<p><img src="self-attention_softmax.png" alt="img" style="zoom:80%;"></p>
<p>这个softmax分数决定了在计算当前位置单词时，其他单词受到的关注的大小。很显然，当前位置会有最高的softmax分数，但有时候也会注意到与当前单词相关的另一个单词。</p>
<p>第五步是将每个value向量乘以注意力分数（准备把它们加起来）。这么做的目的是留下我们想要关注的单词的value（注意力分数高的单词），并且舍弃掉不太相关的单词（它们的注意力分数会比较低，比如乘上0.001这种小数字）。</p>
<p>第六步是将上一步的结果相加，这会产生当前位置（现在来说的话就是第一个词Thinking）的自注意力层结果：</p>
<p><img src="self-attention-output.png" alt="img" style="zoom:80%;"></p>
<p>这就是自注意力的计算。计算的结果向量可以直接传递给前馈神经网络。然而，为了更快速的计算，实际的计算是以矩阵的形式完成的。下面我们来看看矩阵是怎么算的。</p>
<h2 id="matrix-calculation-of-self-attention">Matrix Calculation of
Self-Attention</h2>
<p>第一步是计算query、key和value矩阵。我们直接把所有的嵌入打包成一个矩阵<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="1.928ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 852 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g></g></g></svg></mjx-container></span>，然后乘上我们训练的三个权重矩阵<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="13.981ex" height="2.386ex" role="img" focusable="false" viewBox="0 -860.8 6179.6 1054.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1136.2,363) scale(0.707)"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g></g><g data-mml-node="mo" transform="translate(1745.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msup" transform="translate(2190.2,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1136.2,363) scale(0.707)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><g data-mml-node="mo" transform="translate(4005,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msup" transform="translate(4449.7,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1136.2,363) scale(0.707)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></g></svg></mjx-container></span>：</p>
<p><img src="self-attention-matrix-calculation.png" alt="img" style="zoom:80%;"></p>
<p>最后，因为我们使用的是矩阵，所以可以把上面的第二到第六步都压缩到一个公式来求取自注意力层的输出：</p>
<p><img src="self-attention-matrix-calculation-2.png" alt="img" style="zoom:80%;"></p>
<h2 id="the-beast-with-many-heads">The Beast With Many Heads</h2>
<p>论文进一步通过增加所谓的多头注意力机制来改进自注意力层。这样做有两个好处：</p>
<ol type="1">
<li>它拓展了模型关注不同位置的能力。在上面的例子中，z1多多少少受到了每个其他单词的影响，但是它还是被自身对应的单词所支配。如果我们想要翻译一句像“The
animal didn't cross the street because it was too
tired”的话，那么这样的模型会知道"it"在指什么。</li>
<li>它给予了注意力层许多的“表示子空间”。我们将知道，这是指有不止一对Query、Key和Value的权重矩阵（在Transformer中，使用了八个注意力头，因此会得到八个结果）。每一个权重矩阵都是随机初始化的。在训练后，每一个权重矩阵对会把输入嵌入（或来自低层编码器/解码器的输出）投影到不同的表示子空间。</li>
</ol>
<p><img src="transformer_attention_heads_qkv.png" alt="img" style="zoom: 67%;"></p>
<p>如果我们像上图这样做相同的自注意力计算，也就是用不同的权重矩阵做八次运算，会得到八个不同的Z矩阵：</p>
<p><img src="transformer_attention_heads_z.png" alt="img" style="zoom:67%;"></p>
<p>但这会引出一些问题。前馈神经网络是无法接受八个矩阵作为输入的，它只想接收一个矩阵（一个向量一个单词）。所以我们需要一个可以把八个矩阵合并成一个矩阵的方法。</p>
<p>该怎么做呢？我们可以把这些矩阵连起来，然后乘上一个额外的权重矩阵<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="3.904ex" height="1.997ex" role="img" focusable="false" viewBox="0 -860.8 1725.7 882.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1136.2,363) scale(0.707)"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g></g></g></g></svg></mjx-container></span>：</p>
<p><img src="transformer_attention_heads_weight_matrix_o.png" alt="img" style="zoom: 67%;"></p>
<p>这就是多头注意力的全部内容了，这矩阵可真够多的。让我们试着把所有内容放到一张图看一下：</p>
<p><img src="transformer_multi-headed_self-attention-recap.png" alt="img" style="zoom:67%;"></p>
<p>既然已经介绍完多头注意力，让我们看看之前的例子中，不同的注意力头在编码“it”的时候在关注什么：</p>
<figure>
<img src="transformer_self-attention_visualization_2.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>如果我们把所有的注意力头都放到图上，事情好像变得更复杂了呃：</p>
<figure>
<img src="transformer_self-attention_visualization_3.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="representing-the-order-of-the-sequence-using-positional-encoding">Representing
The Order of The Sequence Using Positional Encoding</h2>
<p>我们现在都还没说如何表示输入序列中词的顺序。</p>
<p>为了解决这个问题，Transformer给每个输入的嵌入都加上了一个向量。这些向量遵循某种特定的模式，可以帮助模型确定每个词的位置或者不同词在句子中的相对距离。我们可以很直觉地认为，加上这些向量后，一旦嵌入向量被投影到Q、K、V的向量空间中并进行点积注意力时，它们会提供有意义的距离信息。</p>
<p><img src="transformer_positional_encoding_vectors.png" alt="img" style="zoom:67%;"></p>
<p>位置编码和输入嵌入具有相同的维度，比如都用四个方格表示：</p>
<p><img src="transformer_positional_encoding_example.png" alt="img" style="zoom:80%;"></p>
<p>那么位置编码到底遵循着什么模式呢？</p>
<p>在下面的图中，每一行对应着一个向量的位置编码，所以第一行就是我们输入序列中的第一个词嵌入的位置编码。每一行有512个值（因为输入嵌入是512维），每个值都介于-1到1之间。我们给编码进行了上色从而更好地可视化：</p>
<p><img src="transformer_positional_encoding_large_example.png" alt="img" style="zoom:67%;"></p>
<p>在论文的3.5节可以看到位置编码的具体公式。我们可以在<a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensor2tensor/blob/23bd23b9830059fbc349381b70d9429b5c40a139/tensor2tensor/layers/common_attention.py"><code>get_timing_signal_1d()</code></a>方法中看到实现。这并不是位置编码的唯一方法，但它的优点在于可以扩展到没见过的的序列长度（比如我们的模型被要求翻译一个比我们训练集中任何句子都要长的句子）。</p>
<h2 id="the-residuals">The Residuals</h2>
<p>在继续往下讲之前，我们需要注意编码器的一个细节，那就是每个子层都有一个残差连接，之后是一个layer-normalization。</p>
<p><img src="transformer_resideual_layer_norm.png" alt="img" style="zoom:80%;"></p>
<p>如果我们将自注意力间的向量和层归一化操作进行可视化，将会是下面的结果：</p>
<p><img src="transformer_resideual_layer_norm_2.png" alt="img" style="zoom:80%;"></p>
<p>当然在解码器的子层中也是这样。如果我们像画一个具有两个编码器和解码器的Transformer，就是下图：</p>
<p><img src="transformer_resideual_layer_norm_3.png" alt="img" style="zoom:67%;"></p>
<h2 id="the-decoder-side">The Decoder Side</h2>
<p>现在我们已经介绍完了encoder的大部分概念，所以我们同样也知道了deocder的各个部分是如何工作的。但我们需要关注它们是如何一起工作的。</p>
<p>编码器首先处理输入序列，编码器的输出会被转化为一组K和V的注意力向量对。这个注意力向量对会被传递给每个解码器的"encoder-decoder
attention"层，这有助于解码器把注意力放在输入序列的合适位置。</p>
<p><img src="transformer_decoding_1.gif" alt="img" style="zoom:80%;"></p>
<p>输出步骤会一直重复，直到遇到一个表示句子结束的特殊符号，表明transformer解码器已完成输出。每一步的输出都会在下一个时间步传递给最底层的解码器，且每一次的输出只是一个词。和编码器相同，在解码器中我们也会添加位置编码来表示每个单词的位置。</p>
<p><img src="transformer_decoding_2.gif" alt="img" style="zoom:80%;"></p>
<p>解码器的自注意力层和解码器中的有一些不同。</p>
<p>在解码器中，我们只允许注意力层看到输出序列的较早的位置（已输出位置的信息）。实现的方式是在自注意力层的softmax之前将未来的输出位置进行掩膜（设置为-inf）。</p>
<p>“encoder-decoder
attention“层的工作原理和前边的多头自注意力差不多，但是Q、K、V的来源不用，Q是从下层创建的（比如解码器的输入和下层decoder组件的输出），但是其K和V是来自编码器最后一个组件的输出结果。</p>
<blockquote>
<p>编码器是对整个输入序列进行编码，然后将其结果转化为K和V传给解码器，这个K、V包含了整个句子的所有信息，但解码器的输入是什么？是拼接之前解码器的输出单词，所以解码器造出来的Q仅包含已经输出的内容。</p>
</blockquote>
<h2 id="the-final-linear-and-softmax-layer">The Final Linear and Softmax
Layer</h2>
<p>解码器最后的输出是一个浮点向量，如何把它变成一个单词呢？那就是最后一个线性层的工作了，我们称之为Softmax层。</p>
<p>线性层就是一个简单的全连接神经网络，它将解码层输出的向量投影成一个更大的向量，称之为logits向量。</p>
<p>我们假设我们的模型从训练集学习了10000个不同的英语单词（训练词汇表），那么logits向量的维度也是10000，每一维对应一个单词的分数。</p>
<p>之后，softmax层将这些分数转化成概率（即归一化，全部为正数，相加为一）。具有最大概率的单词会被选择，然后作为当前时间步的输出。</p>
<figure>
<img src="transformer_decoder_output_softmax.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="recap-of-training">Recap Of Training</h2>
<p>现在我们已经了解了整个Transformer的前向传播过程了，再来看一下训练过程。</p>
<p>在训练期间，未训练的模型会进行同样的前向传播。由于我们是在有标记的训练集上进行训练，所以我们可以将其输出与实际的输出进行比较。</p>
<p>为了可视化这个过程，我们假设我们的词汇表纸包含六个单词（a，am，i，thanks，student和<eos>）。</eos></p>
<p><img src="vocabulary.png" alt="img" style="zoom:67%;"></p>
<p>一旦我们定义了我们的输出词汇表，我们就可以用长度相同的向量来表示词汇表中的词了，这被称为独热编码。例如，我们可以用下面的向量来定义“am”：</p>
<p><img src="one-hot-vocabulary-example.png" alt="img" style="zoom:67%;"></p>
<p>接下来我们讨论一下这个模型的损失函数，损失函数是训练阶段优化模型的指标，通过损失函数，可以帮助我们得到一个更好的模型。</p>
<h2 id="the-loss-function">The Loss Function</h2>
<p>假设我们正在训练我们的模型，并且现在是训练的第一步。我们正在训练一个简单的例子：把“merci”翻译成“thanks”。这意味着，我们希望我输出的概率分布可以指向单词“thanks”。但因为这个模型还没有经过训练，所以这不太可能发生。</p>
<p><img src="transformer_logits_output_and_label.png" alt="img" style="zoom: 67%;"></p>
<p>我们如何比较两种概率分布？这个例子里我们只是将两者相减。更多关于损失函数的介绍，请看<a target="_blank" rel="noopener" href="https://colah.github.io/posts/2015-09-Visual-Information/">cross-entropy</a>和<a target="_blank" rel="noopener" href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">Kullback-Leibler
divergence</a>。</p>
<p>上面只是一个非常简单的例子。更实际一些，我们使用一个短句子。比如输入是“je
suis étudiant”，希望输出是"i am a
student"。这个时候模型的输出就不应该是一个词的概率分布了，能不能连续输出概率分布，最好满足以下要求：</p>
<ul>
<li>每个概率分布向量长度都和词汇表长度相同。我们的例子中是6，实际应用中可能是30000或50000；</li>
<li>第一个概率分布应该在与单词“i”相关的位置上具有最高的概率；</li>
<li>第二个概率分布应该在与单词“am”相关的位置上具有最高的概率；</li>
<li>以此类推，直到第五个输出表示<end of="" sentence="">符号，而这个符号也应该在词汇表中占有一席之地。</end></li>
</ul>
<figure>
<img src="output_target_probability_distributions.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>在经过足够数据和事件的训练后，我们希望会产生下面的概率分布：</p>
<figure>
<img src="output_trained_model_probability_distributions.png" alt="img">
<figcaption aria-hidden="true">img</figcaption>
</figure>
<p>因为模型一次产生一个输出，我们可以认为模型会取出概率分布最大的单词并舍弃其余的。这是一种方法，也叫贪心算法。另一种方法是束搜索(beam
search)，它会保留概率最高的两个单词，然后在下一步继续选择两个概率最高的值，以此类推，这里我们把束搜索的宽度设置为2。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://Ryan-Nightwish.github.io">Ryan Wong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://ryan-nightwish.github.io/2023/05/11/Illustrated-Transformer%E7%BF%BB%E8%AF%91/">http://ryan-nightwish.github.io/2023/05/11/Illustrated-Transformer%E7%BF%BB%E8%AF%91/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://Ryan-Nightwish.github.io" target="_blank">Skylark Space</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87/">论文</a></div><div class="post_share"><div class="social-share" data-image="/img/default_cover.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/05/27/ResNet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="ResNet论文阅读"><img class="cover" src="/img/default_cover.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">ResNet论文阅读</div></div></a></div><div class="next-post pull-right"><a href="/2023/05/08/%E8%BD%AF%E4%BB%B6%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/" title="软件项目管理课程笔记"><img class="cover" src="/img/default_cover.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">软件项目管理课程笔记</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/04/25/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" title="对比学习论文笔记"><img class="cover" src="/img/default_cover.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-25</div><div class="title">对比学习论文笔记</div></div></a></div><div><a href="/2023/05/27/ResNet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="ResNet论文阅读"><img class="cover" src="/img/default_cover.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-27</div><div class="title">ResNet论文阅读</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Ryan Wong</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Ryan-Nightwish" target="_blank" title="Github"><i class="fab fa-github" style="color: #hdhfbb;"></i></a><a class="social-icon" href="mailto:718102754@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #000000;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#the-illustrated-transformer%E7%BF%BB%E8%AF%91"><span class="toc-number">1.</span> <span class="toc-text">The Illustrated
Transformer翻译</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#a-high-level-look"><span class="toc-number">1.1.</span> <span class="toc-text">A High-Level Look</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bringing-the-tensors-into-the-picture"><span class="toc-number">1.2.</span> <span class="toc-text">Bringing The Tensors Into
The Picture</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#now-were-encoding"><span class="toc-number">1.3.</span> <span class="toc-text">Now We&#39;re Encoding！</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#self-attention-at-a-high-level"><span class="toc-number">1.4.</span> <span class="toc-text">Self-Attention at a High
Level</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#self-attention-in-detail"><span class="toc-number">1.5.</span> <span class="toc-text">Self-Attention in Detail</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#matrix-calculation-of-self-attention"><span class="toc-number">1.6.</span> <span class="toc-text">Matrix Calculation of
Self-Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-beast-with-many-heads"><span class="toc-number">1.7.</span> <span class="toc-text">The Beast With Many Heads</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#representing-the-order-of-the-sequence-using-positional-encoding"><span class="toc-number">1.8.</span> <span class="toc-text">Representing
The Order of The Sequence Using Positional Encoding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-residuals"><span class="toc-number">1.9.</span> <span class="toc-text">The Residuals</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-decoder-side"><span class="toc-number">1.10.</span> <span class="toc-text">The Decoder Side</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-final-linear-and-softmax-layer"><span class="toc-number">1.11.</span> <span class="toc-text">The Final Linear and Softmax
Layer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#recap-of-training"><span class="toc-number">1.12.</span> <span class="toc-text">Recap Of Training</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#the-loss-function"><span class="toc-number">1.13.</span> <span class="toc-text">The Loss Function</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/06/18/MAE%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="MAE论文阅读笔记"><img src="/img/default_cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MAE论文阅读笔记"/></a><div class="content"><a class="title" href="/2023/06/18/MAE%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="MAE论文阅读笔记">MAE论文阅读笔记</a><time datetime="2023-06-18T10:24:09.000Z" title="发表于 2023-06-18 18:24:09">2023-06-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/15/%E6%9C%8D%E5%8A%A1%E5%BC%80%E5%8F%91%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E7%82%B9/" title="服务开发技术知识点"><img src="/img/default_cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="服务开发技术知识点"/></a><div class="content"><a class="title" href="/2023/06/15/%E6%9C%8D%E5%8A%A1%E5%BC%80%E5%8F%91%E6%8A%80%E6%9C%AF%E7%9F%A5%E8%AF%86%E7%82%B9/" title="服务开发技术知识点">服务开发技术知识点</a><time datetime="2023-06-15T01:47:23.000Z" title="发表于 2023-06-15 09:47:23">2023-06-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/15/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="软件测试复习笔记"><img src="/img/default_cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="软件测试复习笔记"/></a><div class="content"><a class="title" href="/2023/06/15/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="软件测试复习笔记">软件测试复习笔记</a><time datetime="2023-06-15T01:47:23.000Z" title="发表于 2023-06-15 09:47:23">2023-06-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/15/web%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E7%9F%A5%E8%AF%86%E7%82%B9/" title="web数据管理复习"><img src="/img/default_cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="web数据管理复习"/></a><div class="content"><a class="title" href="/2023/06/15/web%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86%E7%9F%A5%E8%AF%86%E7%82%B9/" title="web数据管理复习">web数据管理复习</a><time datetime="2023-06-15T01:47:23.000Z" title="发表于 2023-06-15 09:47:23">2023-06-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/27/ResNet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="ResNet论文阅读"><img src="/img/default_cover.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ResNet论文阅读"/></a><div class="content"><a class="title" href="/2023/05/27/ResNet%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="ResNet论文阅读">ResNet论文阅读</a><time datetime="2023-05-27T11:21:25.000Z" title="发表于 2023-05-27 19:21:25">2023-05-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Ryan Wong</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>